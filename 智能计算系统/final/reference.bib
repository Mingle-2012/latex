@article{mainpaper,
  author    = {Wang, Hao and Zhang, Lei},
  title     = {A Fast Post-Training Pruning Framework for Transformers},
  journal   = {arXiv preprint arXiv:2301.13014},
  year      = {2023}
}

@article{kitaev2020reformer,
  author    = {Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
  title     = {Reformer: The Efficient Transformer},
  journal   = {arXiv preprint arXiv:2001.04451},
  year      = {2020}
}

@article{wang2020linformer,
  author    = {Sinong Wang and Belinda Li and Madian Khabsa and Han Fang and Hao Ma},
  title     = {Linformer: Self-Attention with Linear Complexity},
  journal   = {arXiv preprint arXiv:2006.04768},
  year      = {2020}
}

@inproceedings{wang2021spatten,
  author    = {Hanrui Wang and Zhekai Zhang and Song Han},
  title     = {Spatten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning},
  booktitle = {2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages     = {97--110},
  year      = {2021},
  publisher = {IEEE}
}

@inproceedings{ham2020a3,
  author    = {Tae Jun Ham and Sung Jun Jung and Seonghak Kim and Young H. Oh and Yeonhong Park and Yoonho Song and Jung-Hun Park and Sanghee Lee and Kyoung Park and Jae W. Lee and others},
  title     = {A$^3$: Accelerating Attention Mechanisms in Neural Networks with Approximation},
  booktitle = {2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages     = {328--341},
  year      = {2020},
  publisher = {IEEE}
}

@article{sanh2019distilbert,
  author    = {Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  title     = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  journal   = {arXiv preprint arXiv:1910.01108},
  year      = {2019}
}

@article{jiao2020tinybert,
  author    = {Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
  title     = {TinyBERT: Distilling BERT for Natural Language Understanding},
  journal   = {arXiv preprint arXiv:1909.10351},
  year      = {2020}
}

@article{zafrir2019q8bert,
  author    = {Ofir Zafrir and Guy Boudoukh and Peter Izsak and Moshe Wasserblat},
  title     = {Q8BERT: Quantized 8Bit BERT},
  journal   = {arXiv preprint arXiv:1910.06188},
  year      = {2019}
}

@article{kim2021ibert,
  author    = {Sehoon Kim and Amir Gholami and Zhewei Yao and Michael W. Mahoney and Kurt Keutzer},
  title     = {I-BERT: Integer-only BERT Quantization},
  journal   = {arXiv preprint arXiv:2101.01321},
  year      = {2021}
}

@article{michel2019heads,
  author    = {Paul Michel and Omer Levy and Graham Neubig},
  title     = {Are Sixteen Heads Really Better than One?},
  journal   = {arXiv preprint arXiv:1905.10650},
  year      = {2019}
}

@article{fan2019reducing,
  author    = {Angela Fan and Edouard Grave and Armand Joulin},
  title     = {Reducing Transformer Depth on Demand with Structured Dropout},
  journal   = {arXiv preprint arXiv:1909.11556},
  year      = {2019}
}

@article{yao2021mlpruning,
  author    = {Zhewei Yao and Linjian Ma and Sheng Shen and Kurt Keutzer and Michael W. Mahoney},
  title     = {MLPruning: A Multilevel Structured Pruning Framework for Transformer-Based Models},
  journal   = {arXiv preprint arXiv:2105.14636},
  year      = {2021}
}

@article{gale2019state,
  author    = {Trevor Gale and Erich Elsen and Sara Hooker},
  title     = {The State of Sparsity in Deep Neural Networks},
  journal   = {arXiv preprint arXiv:1902.09574},
  year      = {2019}
}

@inproceedings{molchanov2019importance,
  author    = {Pavlo Molchanov and Arun Mallya and Stephen Tyree and Iuri Frosio and Jan Kautz},
  title     = {Importance Estimation for Neural Network Pruning},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {11264--11272},
  year      = {2019}
}

@article{frankle2018lottery,
  author    = {Jonathan Frankle and Michael Carbin},
  title     = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  journal   = {arXiv preprint arXiv:1803.03635},
  year      = {2018}
}

@article{nagel2020up,
  author    = {Markus Nagel and Rana Ali Amjad and Mart Van Baalen and Christos Louizos and Tijmen Blankevoort},
  title     = {Up or Down? Adaptive Rounding for Post-Training Quantization},
  journal   = {arXiv preprint arXiv:2004.10568},
  year      = {2020}
}

@article{yvinec2021red,
  author    = {Edouard Yvinec and Arnaud Dapogny and Matthieu Cord and Kevin Bailly},
  title     = {RED: Looking for Redundancies for Data-Free Structured Compression of Deep Neural Networks},
  journal   = {Advances in Neural Information Processing Systems},
  volume    = {34},
  pages     = {20863--20873},
  year      = {2021}
}

@article{kim2020neuron,
  author    = {Woojeong Kim and Suhyun Kim and Mincheol Park and Geonseok Jeon},
  title     = {Neuron Merging: Compensating for Pruned Neurons},
  journal   = {arXiv preprint arXiv:2010.13160},
  year      = {2020}
}

@article{hendrycks2016gaussian,
  author    = {Dan Hendrycks and Kevin Gimpel},
  title     = {Gaussian Error Linear Units (GELUs)},
  journal   = {arXiv preprint arXiv:1606.08415},
  year      = {2016}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{fukushima1980neocognitron,
  title={Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
  author={Fukushima, Kunihiko},
  journal={Biological cybernetics},
  volume={36},
  number={4},
  pages={193--202},
  year={1980},
  publisher={Springer}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, A},
  journal={arXiv preprint arXiv:1912.01703},
  year={2019}
}

@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}

@inproceedings{devlin2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019}
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{rajpurkar2018know,
  title={Know what you don't know: Unanswerable questions for SQuAD},
  author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  journal={arXiv preprint arXiv:1806.03822},
  year={2018}
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}